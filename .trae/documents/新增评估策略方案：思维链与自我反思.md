# 新增评估策略方案

为了更全面地评估大模型在城市更新文献提取任务中的表现，除了现有的 Single（单次）、Stepwise（分步独立）、Stepwise Long（分步长上下文）三种策略外，我建议增加以下几种新策略，从**推理深度、自我反思、知识增强**等维度进行探索。

## 1. CoT (Chain-of-Thought) 思维链策略
*   **核心机制**：要求模型在输出最终结论前，先生成“推理过程”。
*   **具体实现**：
    *   在 System Prompt 中加入指令：“Please think step by step. First analyze the paper's focus, then decide the labels.”
    *   **输出格式**：先输出一段 `<thinking>...</thinking>` 或 `Reasoning: ...`，再输出最终的 TAB 分隔结果。
*   **目的**：验证“让模型显式推理”能否提高分类准确率（尤其是针对模糊案例）。

## 2. Self-Reflection (自我反思/自查) 策略
*   **核心机制**：模型先给出初步答案，然后“自我检查”是否符合定义，最后修正并输出最终答案。
*   **具体实现**：
    *   **Round 1**：给出初步提取结果。
    *   **Round 2**（系统自动追问）："Review your answer against the definition of Urban Renewal. Is this really an existing built-up area intervention? If not, correct it."
    *   **Round 3**：输出最终修正后的结果。
*   **目的**：模拟人类的“检查”过程，减少幻觉和误判。

## 3. RAG-like / Knowledge-Augmented (知识增强) 策略
*   **核心机制**：在 Prompt 中动态插入更详细的“领域知识”或“相似的少样本案例”（Dynamic Few-Shot）。
*   **具体实现**：
    *   如果能预先对数据进行聚类或检索（虽然目前是纯 LLM 实验，但可以模拟“强知识注入”）。
    *   或者：在 Prompt 中强化“负样本”警示（例如：“注意：新建新城不是城市更新”），针对易错点进行**对抗性提示**。
*   **简化版实现**：**Detailed-Definition Strategy**。相比普通 Prompt，使用极其详尽、包含大量正负样本对比的 System Prompt。

## 4. Voting / Ensemble (多路投票) 策略
*   **核心机制**：对同一篇论文进行 3 次独立询问（设置略高的 Temperature），然后对结果进行投票（Majority Vote）。
*   **具体实现**：
    *   代码层面循环调用 3 次 API。
    *   对 3 次的 0/1 结果取众数。
*   **目的**：通过统计一致性来提升鲁棒性。

## 推荐优先实施方案
考虑到实现的边际成本与预期收益，我建议优先实施 **CoT (思维链)** 和 **Self-Reflection (自我反思)** 策略。

### 实施计划
1.  **新建策略类**：
    *   `src/strategies/cot.py`: 实现 CoT 逻辑。
    *   `src/strategies/reflection.py`: 实现反思逻辑。
2.  **注册策略**：更新 `Registry`。
3.  **Prompt 更新**：在 `PromptGenerator` 中添加对应的 Prompt 模板。
4.  **运行实验**：对比新策略与基准策略的效果。
