# 优化评估逻辑与报告输出

根据您的需求，我们将对 `evaluate.py` 进行深度改造，实现批量评估和更详尽的报告输出。

## 1. 核心逻辑变更

### 1.1 批量自动评估

* **当前逻辑**：用户手动选择一个 Output 文件和一个 Label 文件进行一对一评估。

* **新逻辑**：

  1. 用户只需选择**任务**（如 `test1`）。
  2. 程序自动扫描该任务下 `output/` 文件夹内的**所有** `.xlsx` 文件（代表不同策略/shot的结果）。
  3. 程序自动定位该任务下 `labels/` 文件夹内的唯一标签文件（如果有多个，默认取第一个或让用户选一次）。
  4. 程序依次对每个 Output 文件与 Label 文件进行比对。

### 1.2 结果命名与存储

* **命名规则**：`Eval_{Strategy}_{Shot}_{Timestamp}.xlsx`。

  * 例如：预测文件是 `cot_zero_20230101.xlsx`，生成的评估报告就是 `Eval_cot_zero_20230101.xlsx`。

* **存储位置**：存放在 `Data/{task_name}/Result/` 下。

### 1.3 报告内容优化（双Sheet结构）

生成的 Excel 报告将包含两个 Sheet：

* **Sheet 1: "详细对比" (Detail Comparison)**

  * 存放合并后的原始数据。

  * 包含：Title, Abstract。

  * 关键列对比：`是否属于城市更新研究_真值` vs `是否属于城市更新研究_预测`，`空间研究_真值` vs `空间研究_预测`。

  * **高亮/标记**：增加一列 `Is_Error` (0/1) 或 `Diff_Type`，方便您直接筛选出预测错误的行。

* **Sheet 2: "质量评估" (Quality Metrics)**

  * 存放统计指标。

  * 包含：Accuracy, Precision, Recall, F1-Score（如果可能），Correct Count, Total Count。

  * 针对每个评估字段（城市更新、空间研究等）各有一行。

## 2. 代码修改计划 (`evaluate.py`)

### 2.1 函数 `evaluate_single_file`

* 将核心评估逻辑封装为一个函数，接受 `pred_file` 和 `truth_df` 作为输入。

* 返回生成的 Report DataFrame 和 Metrics DataFrame。

### 2.2 主流程 `evaluate_task`

* 列出 `output/` 下所有文件。

* 读取 `labels/` 下的真值文件（一次性读取）。

* 循环遍历 output 文件，调用 `evaluate_single_file`。

* 保存结果到 `Result/`。

### 2.3 增强的 `compute_metrics`

* 除了 Accuracy，尽量计算出 Confusion Matrix 的四个值（TP, TN, FP, FN），以便您了解模型是倾向于误报还是漏报。

## 3. 实施步骤

1. **重构** **`evaluate.py`**：

   * 实现批量扫描逻辑。

   * 实现双 Sheet 输出逻辑。

   * 实现差异列生成逻辑。
2. **测试**：使用现有的 `test2 `任务数据进行验证。

