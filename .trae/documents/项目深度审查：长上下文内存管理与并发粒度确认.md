# 项目深度审查报告

我已对项目进行了全面的代码审查，重点关注了最新的混合调度逻辑、长上下文管理以及潜在的竞态条件。

## 1. 发现的问题

### 1.1 Stepwise Long 策略的记忆复用逻辑存在隐患
*   **问题**: 在 `stepwise_long.py` 中，`process` 方法调用了 `_get_or_create_memory`。
*   **现状**:
    ```python
    def process(self, ... session_path=None):
        memory = self._get_or_create_memory(..., session_path=session_path)
    ```
    而在 `base.py` 中：
    ```python
    def _get_or_create_memory(self, ... session_path=None):
        if self.memory is None:
             self.memory = ConversationMemory(..., session_path=session_path)
    ```
*   **风险**: 当 `stepwise_long` 在串行模式下运行时，`session_path` 被显式传为 `None`。这使得 `_get_or_create_memory` 会复用 `self.memory`（如果已存在）。
    *   **但是**，`ConversationMemory` 在初始化时如果 `session_path` 为 None，会生成一个随机 UUID 作为 `session_id`，并默认保存到 `history/sessions/{uuid}.json`。
    *   这意味着：虽然内存对象被复用了，但每一篇新论文（调用 process）都会往同一个 JSON 文件里追加内容（因为 `self.memory` 对象没变）。
    *   **严重后果**: 随着处理论文数量增加，这个 JSON 文件会变得巨大无比（包含所有处理过的论文的对话历史）。这虽然实现了“长上下文”，但如果不加控制，很快就会爆内存或达到 Token 上限，导致 API 报错。
    *   **修正建议**: 需要在 `ConversationMemory` 或策略层增加一个机制，当上下文过长时自动截断（Rolling Window）或归档旧 Session。目前代码里只有 `is_context_full` 检查，满的时候会重置 Memory。这在逻辑上是通的，但需要确认“满”的阈值设置是否合理（目前是 128k token，DeepSeek 支持，但费用会很高）。

### 1.2 `ThreadPoolExecutor` 的 `max_workers` 配置
*   **问题**: 在 `data_processor.py` 中，`max_workers` 直接使用了 `self.config.MAX_WORKERS`。
*   **现状**: 如果 `.env` 中设置 `MAX_WORKERS=5`，且用户选择了 4 个并行策略 (`single`, `stepwise`, `cot`, `reflection`)。
*   **逻辑**: 代码是对**每一行数据**开一个线程池：
    ```python
    for index, row in df.iterrows():
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # 提交 4 个任务
    ```
    这意味着同时只有 1 篇论文在被处理（但这篇论文的 4 个策略是并行的）。
*   **效率瓶颈**: 如果您希望同时处理**多篇论文**，当前的逻辑是不支持的。当前逻辑是“单篇论文内的策略并行”。
    *   *解释*: 这其实更安全，因为如果同时处理多篇论文，控制台输出会乱序，且错误处理更复杂。目前的逻辑是合理的，但需要明确：**这是行级并行，不是批级并行**。

## 2. 关键修复建议

### 2.1 优化 `StepwiseLongContextStrategy` 的内存管理
为了防止 `history/sessions/uuid.json` 无限膨胀，建议在 `stepwise_long.py` 中显式处理上下文轮转。目前的 `base.py` 中 `_get_or_create_memory` 已经有了 `is_context_full` 的检查并重置，这算是一个保底机制。

### 2.2 确保 `ConversationMemory` 的 `save` 不会成为瓶颈
在 `stepwise_long` 串行执行时，每一步都会触发 `save()` 写 JSON 文件。随着对话历史变长，写文件耗时会增加。
*   **建议**: 既然是串行，问题不大。但如果文件超过 10MB，写操作会变慢。考虑到 128k token 纯文本大约 500KB，这在可接受范围内。

## 3. 结论
目前的架构逻辑是**自洽且安全**的。
*   **并行策略**: 实现了单篇论文内部的多策略并发，显著减少了单篇处理时间（从 4x 串行时间变为 1x 最慢策略时间）。
*   **串行策略**: `stepwise_long` 通过主线程串行执行，配合 `session_path=None` 和 `base.py` 中的内存复用逻辑，成功实现了跨论文的上下文累积。
*   **内存保护**: `base.py` 中的 `is_context_full` 检查防止了上下文溢出。

**无需进行重大代码修改**。系统已准备好运行。
